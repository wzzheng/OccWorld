<!doctype html>
<html>
<head>
<title>OccWorld</title>
<meta name="viewport" content="width=device-width,initial-scale=1">
<!-- <link href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous"> -->
<link href="bootstrap.min.css" rel="stylesheet">
<!-- <script src="https://code.jquery.com/jquery-3.2.1.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script> -->
<!-- <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet"> -->
<link href="opensans.css" rel="stylesheet">
<link rel="icon" href="images/logo3.png">
<link href="style.css" rel="stylesheet">
<style>
  .container_2{
    position: relative;
    width: 100%;
    height: 0;
    padding-bottom: 56.25%;
}
.video {
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
}

  .collapsible {
    background-color: #777;
    color: white;
    cursor: pointer;
    padding: 18px;
    width: 100%;
    border: none;
    text-align: left;
    outline: none;
    font-size: 15px;
  }

  .active, .collapsible:hover {
    background-color: #555;
  }
  
  .content {
    padding: 0 18px;
    max-height: 0;
    overflow: hidden;
    transition: max-height 0.2s ease-out;
    background-color: #f1f1f1;
  }
</style>

<style>
.paperthumb {
  float:left; width: 120px; margin: 3px 10px 7px 0;
}
.paperdesc {
  clear: both;
}
</style>
</head>

<body class="nd-docs">
<div class="nd-pageheader">
 <div class="container">
 <p class="lead" style="font-size:30px">
    <b><a href="https://arxiv.org/abs/2311.16038">OccWorld: Learning a 3D Occupancy World Model for Autonomous Driving</a></b>
  <address style="font-size: 110%;">
    <nobr><a href="https://wzzheng.net/">Wenzhao Zheng</a><sup>*</sup>,</nobr>
    <nobr><a href="https://github.com/chen-wl20">Weiliang Chen</a><sup>*</sup>,</nobr>
    <nobr><a href="https://scholar.google.com/citations?&user=LKVgsk4AAAAJ">Yuanhui Huang</a>,</nobr>
    <nobr><a href="https://boruizhang.site/">Borui Zhang</a>,</nobr>
    <nobr><a href="https://duanyueqi.github.io/">Yueqi Duan</a>,</nobr>
    <nobr><a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
  <br>
      <nobr>Tsinghua University</nobr>
      <!-- <nobr><sup>2</sup>PhiGent Robotics</nobr> -->
  </address>
   <!-- <div style="font-size: 170%;">CVPR 2023</div> -->
  <address style="font-size: 120%;">
	 <!-- <br> -->
  [<a href="https://arxiv.org/pdf/2311.16038"><b>Paper (Arxiv)</b></a>]&nbsp;&nbsp;&nbsp;&nbsp;
  <!-- [<a href="https://www.youtube.com/">Video(Youtube)</a>]&nbsp;&nbsp;&nbsp;&nbsp; -->
  [<a href="https://github.com/wzzheng/OccWorld"><b>Code (GitHub)</b></a>]&nbsp;&nbsp;&nbsp;&nbsp;
  <!-- [<a href="https://zhuanlan.zhihu.com">Post(Zhihu)</a>] -->
  </address>
  <small>*Equal contribution. </small>
 </div>
 </p>
 </div>
</div> <!-- end nd-pageheader -->

<div class="container">


<p align="center">
  <video width="90%" controls>
    <source src="videos/demo_small.mp4" type="video/mp4">
  </video>
</p>

<p align="center">
    <img src="images/teaser.png" width="90%">
</p>
<p><b>Overview of our contributions.</b> Given past 3D occupancy observations, our self-supervised OccWorld trained can forecast future scene evolutions and ego movements jointly. This task requires a spatial understanding of the 3D scene and temporal modeling of how driving scenarios develop. We observe that OccWorld can successfully forecast the movements of surrounding agents and future map elements such as drivable areas. OccWorld even generates more reasonable drivable areas than the ground truth, demonstrating its ability to understand the scene rather than memorizing training data. Still, it fails to forecast new vehicles entering the sight, which is difficult given their absence in the inputs. 
</p>


<!-- <h2>Abstract</h2><hr>
<p>Modern methods for vision-centric autonomous driving perception widely adopt the bird's-eye-view (BEV) representation to describe a 3D scene.
  Despite its better efficiency than voxel representation, it has difficulty describing the fine-grained 3D structure of a scene with a single plane.
  To address this, we propose a tri-perspective view (TPV) representation which accompanies BEV with two additional perpendicular planes.
  We model each point in the 3D space by summing its projected features on the three planes. 
  To lift image features to the 3D TPV space, we further propose a transformer-based TPV encoder (TPVFormer) to obtain the TPV features effectively.
  We employ the attention mechanism to aggregate the image features corresponding to each query in each TPV plane. 
  Experiments show that our model trained with sparse supervision effectively predicts the semantic occupancy for all voxels.
  We demonstrate for the first time that using only camera inputs can achieve comparable performance with LiDAR-based methods on the LiDAR segmentation task on nuScenes.</p> -->

  <h2>OccWorld: a 3D Occupancy World Model</h2><hr>

  <p>
    We adopt a GPT-like generative architecture to predict the next scene from previous scenes in an autoregressive manner.
  </p>

  <p align="center">
    <img src="images/framework.png" width="90%">
  </p>
  
  We adapt GPT to the autonomous driving scenario with two key designs:
  1) We train a 3D occupancy scene tokenizer to produce discrete high-level representations of the 3D scene; 
  2) We perform spatial mixing before and after spatial-wise temporal causal self-attention to efficiently produce globally consistent scene predictions.
  We use ground-truth and predicted scene tokens as inputs for future generations for training and inference, respectively.


<h2>3D Occupancy Scene Tokenizer</h2><hr>
<p>
As the world model operates on the scene representation, its choice is vital to the performance of the world model.
We select it based on three principles:
1) <b>expressiveness</b>. It should be able to comprehensively contain the 3D structural and semantic information of the 3D scene;
2) <b>efficiency</b>. It should be economical to learn (e.g., from weak supervision or self-supervision);
3) <b>versatility</b>. It should be able to adapt to both vision and LiDAR modalities. 
</p>

<p align="center">
    <img src="images/tokenizer.png" width="50%">
</p>
<!-- <p><b>Comparisons of TPV with voxel and BEV representation.</b> 
  While BEV is more efficient than the voxel representation, it discards the height information and cannot comprehensively describe a 3D scene.</p> -->

<p> 
  We use CNNs to encode the 3D occupancy and perform vector quantization to obtain discrete tokens using a learnable codebook.
  We then employ a decoder to reconstruct the input 3D occupancy using the quantized tokens and use a reconstruction objective to train the autoencoder and codebook simultaneously.
 </p>


<h2>Spatial-Temporal Generative Transformer</h2><hr>
<p> 
  Inspired by the remarkable sequential prediction performance of GPT, we adopt a GPT-like autoregressive transformer architecture to instantiate OccWorld.
  Both the spatial relations of world tokens within each time stamp and the temporal relations of tokens across different time stamps should be considered to comprehensively model the world evolution.
Therefore, we We adapt the GPT and propose a spatial-temporal generative transformer architecture to effectively process past world tokens and make predictions of the next future.
</p>

<p align="center">
     <img src="images/transformer.png" width="50%">
</p>

As each scene is composed of numerous world tokens, we adopt spatial mixing modules to model their intrinsic dependencies and obtain multi-scale world tokens to capture multi-level information.
We then perform spatial-wise temporal causal self-attention at each level to forecast the next scene.
We employ a U-net structure to aggregate the multi-scale predictions.


<h2>Results</h2><hr>

<p>
We conduct two tasks to evaluate our OccWorld: 4D occupancy forecasting on the Occ3D dataset and motion planning on the nuScenes dataset.  
</p>

<h4>4D Occupancy Forecasting</h4><hr>

<p>
3D occupancy prediction aims to reconstruct the semantic occupancy for each voxel in the surrounding space, which cannot capture the temporal evolution of the 3D occupancy. 
In this work, we explore the task of 4D occupancy forecasting, which aims to forecast the future 3D occupancy given a few historical occupancy inputs.
</p>


<p>
We evaluated our OccWorld in several settings: OccWorld-O (using ground-truth 3D occupancy), OccWorld-D (using predicted results of <a href="https://github.com/wzzheng/TPVFormer">TPVFormer</a> trained with dense ground-truth 3D occupancy), OccWorld-T (using predicted results of <a href="https://github.com/wzzheng/TPVFormer">TPVFormer</a> trained with sparse semantic LiDAR), and OccWorld-S (using predicted results of <a href="https://github.com/huang-yh/SelfOcc">SelfOcc</a> trained in a self-supervised manner).
</p>

<p align="center">
  <img src="images/4docc.png" width="90%">
</p>

<p>
  We observe that OccWorld-O can generate non-trivial future 3D occupancy with much better results than Copy&Paste, showing that our model learns the underlying scene evolution.
  OccWorld-D, OccWorld-T, and OccWorld-S can be seen as end-to-end vision-based 4D occupancy forecasting methods as they take surrounding images as input.
  This task is very challenging since it requires both 3D structure reconstruction and forecasting.
  It is especially difficult for the self-supervised OccWorld-S, which exploits no 3D occupancy information even during training.
  Still, our OccWorld generates future 3D occupancy with non-trivial mIoU and IoU on the end-to-end setting.
  
 </p> 



<h4>Motion Planning</h4><hr>

<p>
  We compare the motion planning performance of our OccWorld with various settings with state-of-the-art end-to-end autonomous driving methods.
</p>

<p align="center">
  <img src="images/planning.png" width="90%">
</p>

<p>
  Despite the strong performance of UniAD, the additional annotations in the 3D space are very difficult to obtain, making it difficult to scale to large-scale driving data.
  As an alternative, OccWorld demonstrates competitive performance by employing 3D occupancy as the scene representation which can be efficiently obtained by <a href="https://weiyithu.github.io/SurroundOcc/">accumulating LiDAR scans</a>.
</p>


<h4>Visualizations</h4><hr>

<p>
  We visualize the output results of the our OccWorld with different settings.
</p>

<p align="center">
  <img src="images/vis.png" width="90%">
</p>

<p>
  We see that our models can successfully forecast the movements of cars and can complete unseen map elements in the inputs such as drivable areas. 
  The planning trajectory is also more accurate with better 4D occupancy forecasting.
</p>

<p>
<div class="card">
<h3 class="card-header">Bibtex</h3>
<div class="card-block">
<pre class="card-text clickselect">
  @article{zheng2023occworld,
    title={OccWorld: Learning a 3D Occupancy World Model for Autonomous Driving},
    author={Zheng, Wenzhao and Chen, Weiliang and Huang, Yuanhui and Zhang, Borui and Duan, Yueqi and Lu, Jiwen },
    journal={arXiv preprint arXiv: 2311.16038},
    year={2023}
}
</pre>
</div>
</div>
</p>


<p align="right">
     <a href="https://hanlab.mit.edu/projects/anycost-gan/">Website Template</a>
</p>

</div>
</div> <!-- row -->

</div> <!-- container -->

<script>
  var coll = document.getElementsByClassName("collapsible");
  var i;
  
  for (i = 0; i < coll.length; i++) {
    coll[i].addEventListener("click", function() {
      this.classList.toggle("active");
      var content = this.nextElementSibling;
      if (content.style.maxHeight){
        content.style.maxHeight = null;
      } else {
        content.style.maxHeight = content.scrollHeight * 50+ "px";
      } 
      content.style.height = "550%";
    });
  }
</script>

</body>
</html>


